{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a590e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"/Users/monsieur/Jupyter Notebook/br_example_constitution.html\", encoding=\"utf-8\") as f :\n",
    "    \n",
    "    html_source = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_source, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff900bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n"
     ]
    }
   ],
   "source": [
    "title = soup.find('p', {\"id\":\"title\"})\n",
    "contents = soup.find_all('p', {\"id\":\"content\"})\n",
    "\n",
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7a266e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제1조 ①대한민국은 민주공화국이다.②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "제2조 ①대한민국의 국민이 되는 요건은 법률로 정한다.②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n"
     ]
    }
   ],
   "source": [
    "for content in contents :\n",
    "    print(content.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79e19e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Element foun by tag p ====\n",
      "\n",
      "<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
      "\n",
      "==== Elements found by tag br ====\n",
      "\n",
      "<br/>\n",
      "\n",
      "==== Result replaced <br> with \n",
      "\n",
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "html1 = '<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>'\n",
    "soup1 = BeautifulSoup(html1, \"lxml\")\n",
    "\n",
    "print(\"==== Element foun by tag p ====\\n\")\n",
    "content1 = soup1.find('p',{\"id\":\"content\"})\n",
    "print(content1)\n",
    "\n",
    "br_content = content1.find(\"br\")\n",
    "print(\"\\n==== Elements found by tag br ====\\n\")\n",
    "print(br_content)\n",
    "\n",
    "br_content.replace_with(\"\\n\")\n",
    "print(\"\\n==== Result replaced <br> with \\n\")\n",
    "print(content1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ce73ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html1, \"lxml\")\n",
    "content2 = soup2.find(\"p\", {\"id\":\"content\"})\n",
    "\n",
    "br_contents = content2.find_all(\"br\")\n",
    "for br_content in br_contents :\n",
    "    br_content.replace_with(\"\\n\")\n",
    "print(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "edcf07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_newline(soup_html) :\n",
    "    br_to_newlines = soup_html.find_all(\"br\")\n",
    "    for br_to_newline in br_to_newlines :\n",
    "        br_to_newline.replace_with(\"\\n\")\n",
    "    return soup_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad16926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html1, \"lxml\")\n",
    "content2 = soup2.find(\"p\",{\"id\":\"content\"})\n",
    "content3 = replace_newline(content2)\n",
    "print(content3.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc708dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법 \n",
      "\n",
      "제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다. \n",
      "\n",
      "제2조 \n",
      "①대한민국의 국민이 되는 요건은 법률로 정한다.\n",
      "②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_source, \"lxml\")\n",
    "\n",
    "title = soup.find(\"p\",{\"id\":\"title\"})\n",
    "contents = soup.find_all(\"p\", {\"id\":\"content\"})\n",
    "\n",
    "print(title.get_text(), \"\\n\")\n",
    "\n",
    "for content in contents :\n",
    "    content1 = replace_newline(content)\n",
    "    print(content1.get_text(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6393b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ad30b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 웹사이트에서 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7c692370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tistory.com\">Tistory.com</a>,\n",
       " <a href=\"/siteinfo/kakao.com\">Kakao.com</a>,\n",
       " <a href=\"/siteinfo/tmall.com\">Tmall.com</a>,\n",
       " <a href=\"/siteinfo/coupang.com\">Coupang.com</a>,\n",
       " <a href=\"/siteinfo/google.co.kr\">Google.co.kr</a>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = req.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "website_ranking[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "750d5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_ranking_address = [website.get_text() for website in website_ranking[1:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bf057163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tistory.com',\n",
       " 'Kakao.com',\n",
       " 'Tmall.com',\n",
       " 'Coupang.com',\n",
       " 'Google.co.kr']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8da24d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Most visited websites in Korea ==\n",
      "1위 : Google.com\n",
      "2위 : Naver.com\n",
      "3위 : Youtube.com\n",
      "4위 : Daum.net\n",
      "5위 : Tistory.com\n",
      "6위 : Kakao.com\n",
      "7위 : Tmall.com\n",
      "8위 : Coupang.com\n",
      "9위 : Google.co.kr\n",
      "10위 : Amazon.com\n"
     ]
    }
   ],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = req.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "website_ranking_address = [website.get_text() for website in website_ranking[1:]]\n",
    "\n",
    "print(\"== Most visited websites in Korea ==\")\n",
    "for k in range(10) :\n",
    "    print(\"{}위 : {}\".format(k+1, website_ranking_address[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "11cad8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kakao.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tmall.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Coupang.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Google.co.kr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Amazon.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Website\n",
       "1     Google.com\n",
       "2      Naver.com\n",
       "3    Youtube.com\n",
       "4       Daum.net\n",
       "5    Tistory.com\n",
       "6      Kakao.com\n",
       "7      Tmall.com\n",
       "8    Coupang.com\n",
       "9   Google.co.kr\n",
       "10    Amazon.com"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "website_ranking_dict = {\"Website\": website_ranking_address}\n",
    "df = pd.DataFrame(website_ranking_dict, columns=[\"Website\"], index=range(1, len(website_ranking_address)+1))\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "09423b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1위 : Google.com\n",
      "2위 : Naver.com\n",
      "3위 : Youtube.com\n",
      "4위 : Daum.net\n",
      "5위 : Tistory.com\n",
      "6위 : Kakao.com\n",
      "7위 : Tmall.com\n",
      "8위 : Coupang.com\n",
      "9위 : Google.co.kr\n",
      "10위 : Amazon.com\n",
      "11위 : Netflix.com\n",
      "12위 : Sohu.com\n",
      "13위 : Facebook.com\n",
      "14위 : Namu.wiki\n",
      "15위 : Qq.com\n",
      "16위 : Wikipedia.org\n",
      "17위 : Taobao.com\n",
      "18위 : 360.cn\n",
      "19위 : Jd.com\n",
      "20위 : Baidu.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "website_html = requests.get(url).text\n",
    "website_soup = BeautifulSoup(website_html, \"lxml\")\n",
    "\n",
    "website_ranking = website_soup.select('p a')\n",
    "website_address = [website.get_text() for website in website_ranking[1:]]\n",
    "for i in range(20) :\n",
    "    print(\"{0}위 : {1}\".format(i+1, website_address[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "36b95daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most Visited Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kakao.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tmall.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Coupang.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Google.co.kr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Netflix.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sohu.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Namu.wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qq.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Taobao.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>360.cn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jd.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Baidu.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Most Visited Website\n",
       "1            Google.com\n",
       "2             Naver.com\n",
       "3           Youtube.com\n",
       "4              Daum.net\n",
       "5           Tistory.com\n",
       "6             Kakao.com\n",
       "7             Tmall.com\n",
       "8           Coupang.com\n",
       "9          Google.co.kr\n",
       "10           Amazon.com\n",
       "11          Netflix.com\n",
       "12             Sohu.com\n",
       "13         Facebook.com\n",
       "14            Namu.wiki\n",
       "15               Qq.com\n",
       "16        Wikipedia.org\n",
       "17           Taobao.com\n",
       "18               360.cn\n",
       "19               Jd.com\n",
       "20            Baidu.com"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_address_dict = {\"Most Visited Website\":website_address}\n",
    "df = pd.DataFrame(website_address_dict, index=range(1,len(website_address)+1), columns=[\"Most Visited Website\"])\n",
    "df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "44ed9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 웹페이지 이미지 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "cda1db97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이미지 하나만 받기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.python.org/static/img/python-logo.png\"\n",
    "\n",
    "html_image = requests.get(url)\n",
    "html_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b49abcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo.png'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "image_file_name = os.path.basename(url)\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e26d699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/monsieur/Jupyter Notebook/imgdownload'\n",
    "\n",
    "if not os.path.exists(folder) :\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1d1d32db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/monsieur/Jupyter Notebook/imgdownload/python-logo.png'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "33b86486",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFile = open(image_path, \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "989c4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size) :\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "96871a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python-logo.png']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2e277916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"Reshot\" height=\"33\" src=\"https://www.reshot.com/build/reshot-logo--mark-cc49363ac3f7876f854286af4266ead51a7ff9e0fa12f30677c9e758d43dd0d1.svg\" title=\"Reshot\" width=\"46\"/>,\n",
       " <img alt=\"\" class=\"photos-item-card__image\" height=\"2448\" loading=\"lazy\" src=\"https://res.cloudinary.com/twenty20/private_images/t_reshot-400/v1521838685/photosp/bae96789-a5ab-4471-b54f-9686ace09e33/bae96789-a5ab-4471-b54f-9686ace09e33.jpg\" width=\"2448\"/>,\n",
       " <img alt=\"Back off!\" class=\"photos-item-card__image\" height=\"2361\" loading=\"lazy\" src=\"https://res.cloudinary.com/twenty20/private_images/t_reshot-400/v1597098233/photosp/a44357c5-b1c3-41ef-9a65-7a4937b06a44/a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg\" width=\"3148\"/>,\n",
       " <img alt=\"Orphans\" class=\"photos-item-card__image\" height=\"3375\" loading=\"lazy\" src=\"https://res.cloudinary.com/twenty20/private_images/t_reshot-400/v1617578418/photosp/34fd9c70-8996-4706-a0f1-113231ed3eee/34fd9c70-8996-4706-a0f1-113231ed3eee.jpg\" width=\"3375\"/>]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 여러 이미지 받기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.reshot.com/free-stock-photos/search/animal/\"\n",
    "\n",
    "html_reshot_image = requests.get(URL).text\n",
    "soup_reshot_image = BeautifulSoup(html_reshot_image, \"lxml\")\n",
    "reshot_image_elements = soup_reshot_image.select('a img')\n",
    "reshot_image_elements[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3d403fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://res.cloudinary.com/twenty20/private_images/t_reshot-400/v1521838685/photosp/bae96789-a5ab-4471-b54f-9686ace09e33/bae96789-a5ab-4471-b54f-9686ace09e33.jpg'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshot_image_url = reshot_image_elements[1].get('src')\n",
    "reshot_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4043828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_image = requests.get(reshot_image_url)\n",
    "\n",
    "folder = \"/Users/monsieur/Jupyter Notebook/imgdownload\"\n",
    "\n",
    "imageFile = open(os.path.join(folder, os.path.basename(reshot_image_url)), 'wb')\n",
    "\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size) :\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "eb700a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 종합\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def get_image_url(url) :\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")\n",
    "    image_elements = soup_image_url.select('img')\n",
    "    if image_elements != None :\n",
    "        image_urls = []\n",
    "        for image_element in image_elements :\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls\n",
    "    else :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "678a99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image File Name: reshot-logo--mark-cc49363ac3f7876f854286af4266ead51a7ff9e0fa12f30677c9e758d43dd0d1.svg. Download completed!\n",
      "Image File Name: bae96789-a5ab-4471-b54f-9686ace09e33.jpg. Download completed!\n",
      "Image File Name: a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg. Download completed!\n",
      "Image File Name: 34fd9c70-8996-4706-a0f1-113231ed3eee.jpg. Download completed!\n",
      "Image File Name: dbd9fa3b-238b-47b1-8e20-c05400cbe921.jpg. Download completed!\n",
      "Image File Name: 737d192f-ba38-4a71-9bb9-9d40b45d0263.jpg. Download completed!\n",
      "Image File Name: c3c3604d-36eb-4f8a-9768-cebc0749d5a5.jpg. Download completed!\n",
      "Image File Name: bd35932f-98e9-4164-bb40-24f2df77ce93.jpg. Download completed!\n",
      "Image File Name: c20e17c7-9b38-4cef-bd56-75ee6be06a5e.jpg. Download completed!\n",
      "Image File Name: f73263fe-bc01-44b0-9326-4b0ea6e820a7.jpg. Download completed!\n",
      "Image File Name: 59c2a5a3-2958-4e8e-b469-9fcf965e07a3.jpg. Download completed!\n",
      "Image File Name: 7b3e5687-f989-4518-befe-32069f080700.jpg. Download completed!\n",
      "Image File Name: c4a1e6a9-367b-4bff-ac76-78c2c7866e78.jpg. Download completed!\n",
      "Image File Name: ac1ec457-3756-4e2c-ad56-83fcba9dac95.jpg. Download completed!\n",
      "Image File Name: 863464bb-c1eb-4f75-8279-52ea582da2a1.jpg. Download completed!\n",
      "===============================================================\n",
      " ***All selected images are completely downloaded!***\n"
     ]
    }
   ],
   "source": [
    "def download_image(img_folder, img_url) :\n",
    "    if img_url != None :\n",
    "        html_image = requests.get(img_url)\n",
    "        imageFile = open(os.path.join(img_folder, os.path.basename(img_url)), \"wb\")\n",
    "        \n",
    "        chunk_size = 1000000\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            imageFile.write(chunk)\n",
    "            imageFile.close()\n",
    "        print(\"Image File Name: {}. Download completed!\".format(os.path.basename(img_url)))\n",
    "    else :\n",
    "        print(\"No image to download!\")\n",
    "        \n",
    "reshot_url = \"https://www.reshot.com/free-stock-photos/search/animal/\"\n",
    "\n",
    "figure_folder = \"/Users/monsieur/Jupyter Notebook/imgdownload\"\n",
    "\n",
    "reshot_image_urls = get_image_url(reshot_url)\n",
    "\n",
    "num_of_download_image = 15\n",
    "\n",
    "for k in range(num_of_download_image) :\n",
    "    download_image(figure_folder, reshot_image_urls[k])\n",
    "print(\"===============================================================\\n\", \"***All selected images are completely downloaded!***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08e7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa9220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
